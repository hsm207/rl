{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 3), (75,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(n_features=3)\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the forward pass of a linear regression model using Haiku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import haiku as hk\n",
    "\n",
    "def forward(X):\n",
    "    lin = hk.Linear(1)\n",
    "    return lin(X).ravel()\n",
    "\n",
    "forward = hk.transform(forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialze the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "/opt/conda/lib/python3.8/site-packages/jax/_src/numpy/lax_numpy.py:2872: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FlatMapping({\n",
       "  'linear': FlatMapping({\n",
       "              'w': DeviceArray([[0.3705009 ],\n",
       "                                [0.2911511 ],\n",
       "                                [0.56166327]], dtype=float32),\n",
       "              'b': DeviceArray([0.], dtype=float32),\n",
       "            }),\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(seed=13)\n",
    "params = forward.init(rng, X)\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to perform forward pass given input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = forward.apply\n",
    "f(params, rng, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, X, y_true):\n",
    "    y_pred = f(params, rng, X)\n",
    "    err = y_pred - y_true\n",
    "    return jnp.mean(jnp.square(err))\n",
    "\n",
    "grad_fn = jax.grad(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 15677.8184\n",
      "Loss at iteration 1: 9501.0879\n",
      "Loss at iteration 2: 5833.2993\n",
      "Loss at iteration 3: 3624.5974\n",
      "Loss at iteration 4: 2276.9775\n",
      "Loss at iteration 5: 1444.7137\n",
      "Loss at iteration 6: 924.9911\n",
      "Loss at iteration 7: 597.1456\n",
      "Loss at iteration 8: 388.4287\n",
      "Loss at iteration 9: 254.4359\n",
      "Loss at iteration 10: 167.7528\n",
      "Loss at iteration 11: 111.2772\n",
      "Loss at iteration 12: 74.2397\n",
      "Loss at iteration 13: 49.8000\n",
      "Loss at iteration 14: 33.5794\n",
      "Loss at iteration 15: 22.7540\n",
      "Loss at iteration 16: 15.4914\n",
      "Loss at iteration 17: 10.5944\n",
      "Loss at iteration 18: 7.2765\n",
      "Loss at iteration 19: 5.0180\n",
      "Loss at iteration 20: 3.4739\n",
      "Loss at iteration 21: 2.4138\n",
      "Loss at iteration 22: 1.6829\n",
      "Loss at iteration 23: 1.1771\n",
      "Loss at iteration 24: 0.8258\n",
      "Loss at iteration 25: 0.5810\n",
      "Loss at iteration 26: 0.4098\n",
      "Loss at iteration 27: 0.2898\n",
      "Loss at iteration 28: 0.2054\n",
      "Loss at iteration 29: 0.1458\n",
      "Loss at iteration 30: 0.1038\n",
      "Loss at iteration 31: 0.0739\n",
      "Loss at iteration 32: 0.0528\n",
      "Loss at iteration 33: 0.0377\n",
      "Loss at iteration 34: 0.0270\n",
      "Loss at iteration 35: 0.0194\n",
      "Loss at iteration 36: 0.0139\n",
      "Loss at iteration 37: 0.0100\n",
      "Loss at iteration 38: 0.0072\n",
      "Loss at iteration 39: 0.0052\n",
      "Loss at iteration 40: 0.0037\n",
      "Loss at iteration 41: 0.0027\n",
      "Loss at iteration 42: 0.0019\n",
      "Loss at iteration 43: 0.0014\n",
      "Loss at iteration 44: 0.0010\n",
      "Loss at iteration 45: 0.0007\n",
      "Loss at iteration 46: 0.0005\n",
      "Loss at iteration 47: 0.0004\n",
      "Loss at iteration 48: 0.0003\n",
      "Loss at iteration 49: 0.0002\n"
     ]
    }
   ],
   "source": [
    "lr = 10/100\n",
    "\n",
    "for i in range(50):\n",
    "    # show the performance on the test set\n",
    "    loss = loss_fn(params, X_test, y_test)\n",
    "    print(f\"Loss at iteration {i}: {loss:.4f}\")\n",
    "    \n",
    "    # update the gradients wrt loss function using the \n",
    "    # training set\n",
    "    grads = grad_fn(params, X, y)\n",
    "    params = jax.tree_multimap(\n",
    "        lambda p, g: p - lr * g,\n",
    "        params,\n",
    "        grads\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
