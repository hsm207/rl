{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75, 3), (75,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = make_regression(n_features=3)\n",
    "X, X_test, y, y_test = train_test_split(X, y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters of the linear regression i.e. weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'w': DeviceArray([0., 0., 0.], dtype=float32), 'b': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "params = {\n",
    "    'w': jnp.zeros(X.shape[1:]),\n",
    "    'b': 0.0\n",
    "}\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(params, X):\n",
    "    W = params['w']\n",
    "    b = params['b']\n",
    "    return jnp.dot(X, W) + b\n",
    "\n",
    "assert forward(params, X).shape == (75,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function (mse):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decorator to speed things up\n",
    "@jax.jit\n",
    "def loss_fn(params, X, y_true):\n",
    "    y_pred = forward(params, X)\n",
    "    err = y_pred - y_true\n",
    "    return jnp.mean(jnp.square(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to compute the gradient of the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default, jax.grad takes the gradient of loss_fn wrt\n",
    "# loss_fn's first arg i.e. params\n",
    "grad_fn = jax.grad(loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b': DeviceArray(-27.83489, dtype=float32),\n",
       " 'w': DeviceArray([ -74.10313, -186.97112, -158.06946], dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_fn(params, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 13366.9443\n",
      "Loss at iteration 1: 7959.3882\n",
      "Loss at iteration 2: 4775.9360\n",
      "Loss at iteration 3: 2886.5881\n",
      "Loss at iteration 4: 1756.5554\n",
      "Loss at iteration 5: 1075.6941\n",
      "Loss at iteration 6: 662.6248\n",
      "Loss at iteration 7: 410.4019\n",
      "Loss at iteration 8: 255.4695\n",
      "Loss at iteration 9: 159.7708\n",
      "Loss at iteration 10: 100.3560\n",
      "Loss at iteration 11: 63.2931\n",
      "Loss at iteration 12: 40.0712\n",
      "Loss at iteration 13: 25.4617\n",
      "Loss at iteration 14: 16.2350\n",
      "Loss at iteration 15: 10.3866\n",
      "Loss at iteration 16: 6.6668\n",
      "Loss at iteration 17: 4.2928\n",
      "Loss at iteration 18: 2.7730\n",
      "Loss at iteration 19: 1.7969\n",
      "Loss at iteration 20: 1.1680\n",
      "Loss at iteration 21: 0.7617\n",
      "Loss at iteration 22: 0.4982\n",
      "Loss at iteration 23: 0.3270\n",
      "Loss at iteration 24: 0.2153\n",
      "Loss at iteration 25: 0.1422\n",
      "Loss at iteration 26: 0.0942\n",
      "Loss at iteration 27: 0.0626\n",
      "Loss at iteration 28: 0.0418\n",
      "Loss at iteration 29: 0.0280\n",
      "Loss at iteration 30: 0.0188\n",
      "Loss at iteration 31: 0.0126\n",
      "Loss at iteration 32: 0.0085\n",
      "Loss at iteration 33: 0.0058\n",
      "Loss at iteration 34: 0.0039\n",
      "Loss at iteration 35: 0.0027\n",
      "Loss at iteration 36: 0.0018\n",
      "Loss at iteration 37: 0.0013\n",
      "Loss at iteration 38: 0.0009\n",
      "Loss at iteration 39: 0.0006\n",
      "Loss at iteration 40: 0.0004\n",
      "Loss at iteration 41: 0.0003\n",
      "Loss at iteration 42: 0.0002\n",
      "Loss at iteration 43: 0.0001\n",
      "Loss at iteration 44: 0.0001\n",
      "Loss at iteration 45: 0.0001\n",
      "Loss at iteration 46: 0.0000\n",
      "Loss at iteration 47: 0.0000\n",
      "Loss at iteration 48: 0.0000\n",
      "Loss at iteration 49: 0.0000\n"
     ]
    }
   ],
   "source": [
    "lr = 10/100\n",
    "\n",
    "for i in range(50):\n",
    "    # show the performance on the test set\n",
    "    loss = loss_fn(params, X_test, y_test)\n",
    "    print(f\"Loss at iteration {i}: {loss:.4f}\")\n",
    "    \n",
    "    # update the gradients wrt loss function using the \n",
    "    # training set\n",
    "    grads = grad_fn(params, X, y)\n",
    "    params['w'] = params['w'] - lr * grads['w']\n",
    "    params['b'] = params['b'] - lr * grads['b']\n",
    "    \n",
    "    # alternative way to update params\n",
    "#     params = jax.tree_multimap(\n",
    "#         lambda p, g: p - lr * g,\n",
    "#         params,\n",
    "#         grads\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
