{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://coax.readthedocs.io/en/latest/examples/getting_started/third_agent.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import jax\n",
    "import coax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "from optax import adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ.setdefault('JAX_PLATFORM_NAME', 'gpu')     # tell JAX to use GPU\n",
    "# os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.1'  # don't use all gpu mem\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'              # tell XLA to be quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'ppo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.wrappers.AtariPreprocessing(env)\n",
    "env = coax.wrappers.FrameStacking(env, num_frames=3)\n",
    "env = coax.wrappers.TrainMonitor(env, name=name, tensorboard_dir=f\"./data/tensorboard/{name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared(S, is_training):\n",
    "    seq = hk.Sequential([\n",
    "        coax.utils.diff_transform,\n",
    "        hk.Conv2D(16, kernel_shape=8, stride=4), jax.nn.relu,\n",
    "        hk.Conv2D(32, kernel_shape=4, stride=2), jax.nn.relu,\n",
    "        hk.Flatten(),\n",
    "    ])\n",
    "    X = jnp.stack(S, axis=-1) / 255.  # stack frames\n",
    "    return seq(X)\n",
    "\n",
    "\n",
    "def func_pi(S, is_training):\n",
    "    logits = hk.Sequential((\n",
    "        hk.Linear(256), jax.nn.relu,\n",
    "        hk.Linear(env.action_space.n, w_init=jnp.zeros),\n",
    "    ))\n",
    "    X = shared(S, is_training)\n",
    "    return {'logits': logits(X)}\n",
    "\n",
    "\n",
    "def func_v(S, is_training):\n",
    "    value = hk.Sequential((\n",
    "        hk.Linear(256), jax.nn.relu,\n",
    "        hk.Linear(1, w_init=jnp.zeros), jnp.ravel\n",
    "    ))\n",
    "    X = shared(S, is_training)\n",
    "    return value(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the RL problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ppo|MainThread|absl|WARNING] No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "# function approximators\n",
    "pi = coax.Policy(func_pi, env)\n",
    "v = coax.V(func_v, env)\n",
    "\n",
    "# target networks\n",
    "pi_behavior = pi.copy()\n",
    "v_targ = v.copy()\n",
    "\n",
    "# policy regularizer (avoid premature exploitation)\n",
    "entropy = coax.regularizers.EntropyRegularizer(pi, beta=0.001)\n",
    "\n",
    "# updaters\n",
    "simpletd = coax.td_learning.SimpleTD(v, v_targ, optimizer=adam(3e-4))\n",
    "ppo_clip = coax.policy_objectives.PPOClip(pi, regularizer=entropy, optimizer=adam(3e-4))\n",
    "\n",
    "# reward tracer and replay buffer\n",
    "tracer = coax.reward_tracing.NStep(n=5, gamma=0.99)\n",
    "buffer = coax.experience_replay.SimpleReplayBuffer(capacity=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ppo|MainThread|TrainMonitor|INFO] ep: 1,\tT: 807,\tG: -21,\tavg_G: -21,\tt: 806,\tdt: 182.260ms,\tSimpleTD/loss: 0.0565,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: 0.00654\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 2,\tT: 1,896,\tG: -19,\tavg_G: -20,\tt: 1088,\tdt: 104.867ms,\tSimpleTD/loss: 0.037,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: -0.00446\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 3,\tT: 2,802,\tG: -21,\tavg_G: -20.3,\tt: 905,\tdt: 80.122ms,\tSimpleTD/loss: 0.0145,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: -0.00215\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 4,\tT: 3,901,\tG: -19,\tavg_G: -20,\tt: 1098,\tdt: 118.034ms,\tSimpleTD/loss: 0.0111,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.0017\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 5,\tT: 4,861,\tG: -20,\tavg_G: -20,\tt: 959,\tdt: 115.876ms,\tSimpleTD/loss: 0.01,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00467\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 6,\tT: 5,685,\tG: -21,\tavg_G: -20.2,\tt: 823,\tdt: 122.553ms,\tSimpleTD/loss: 0.0054,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00121\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 7,\tT: 6,673,\tG: -20,\tavg_G: -20.1,\tt: 987,\tdt: 78.641ms,\tSimpleTD/loss: 0.00875,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: -0.00121\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 8,\tT: 7,591,\tG: -20,\tavg_G: -20.1,\tt: 917,\tdt: 76.637ms,\tSimpleTD/loss: 0.00859,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: -0.0021\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 9,\tT: 8,587,\tG: -20,\tavg_G: -20.1,\tt: 995,\tdt: 77.871ms,\tSimpleTD/loss: 0.0082,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00144\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 10,\tT: 9,438,\tG: -21,\tavg_G: -20.2,\tt: 850,\tdt: 64.431ms,\tSimpleTD/loss: 0.0121,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: -0.000707\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 11,\tT: 10,340,\tG: -21,\tavg_G: -20.3,\tt: 901,\tdt: 127.940ms,\tSimpleTD/loss: 0.0072,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: 0.00103\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 12,\tT: 11,470,\tG: -18,\tavg_G: -20.1,\tt: 1129,\tdt: 128.970ms,\tSimpleTD/loss: 0.00986,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.000962\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 13,\tT: 12,578,\tG: -21,\tavg_G: -20.1,\tt: 1107,\tdt: 129.639ms,\tSimpleTD/loss: 0.0179,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: 0.00809\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 14,\tT: 13,614,\tG: -19,\tavg_G: -20,\tt: 1035,\tdt: 125.398ms,\tSimpleTD/loss: 0.0148,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00151\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 15,\tT: 14,463,\tG: -21,\tavg_G: -20.1,\tt: 848,\tdt: 131.238ms,\tSimpleTD/loss: 0.00801,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: -0.00109\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 16,\tT: 15,274,\tG: -21,\tavg_G: -20.2,\tt: 810,\tdt: 121.483ms,\tSimpleTD/loss: 0.0084,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00245\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 17,\tT: 16,094,\tG: -21,\tavg_G: -20.3,\tt: 819,\tdt: 139.583ms,\tSimpleTD/loss: 0.00464,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.000853\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 18,\tT: 17,021,\tG: -20,\tavg_G: -20.3,\tt: 926,\tdt: 151.264ms,\tSimpleTD/loss: 0.00998,\tEntropyRegularizer/entropy: 1.79,\tPPOClip/loss: 3.28e-05\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 19,\tT: 17,905,\tG: -20,\tavg_G: -20.2,\tt: 883,\tdt: 136.569ms,\tSimpleTD/loss: 0.0119,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00426\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 20,\tT: 18,800,\tG: -20,\tavg_G: -20.2,\tt: 894,\tdt: 152.773ms,\tSimpleTD/loss: 0.00749,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00272\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 21,\tT: 20,031,\tG: -19,\tavg_G: -20.1,\tt: 1230,\tdt: 145.072ms,\tSimpleTD/loss: 0.0174,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00438\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 22,\tT: 20,939,\tG: -21,\tavg_G: -20.2,\tt: 907,\tdt: 140.347ms,\tSimpleTD/loss: 0.011,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00209\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 23,\tT: 22,169,\tG: -18,\tavg_G: -20,\tt: 1229,\tdt: 144.249ms,\tSimpleTD/loss: 0.0143,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00336\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 24,\tT: 23,153,\tG: -20,\tavg_G: -20,\tt: 983,\tdt: 143.741ms,\tSimpleTD/loss: 0.0131,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00247\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 25,\tT: 23,935,\tG: -21,\tavg_G: -20.1,\tt: 781,\tdt: 142.881ms,\tSimpleTD/loss: 0.00976,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00134\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 26,\tT: 24,908,\tG: -21,\tavg_G: -20.2,\tt: 972,\tdt: 143.574ms,\tSimpleTD/loss: 0.0124,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00244\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 27,\tT: 25,723,\tG: -21,\tavg_G: -20.2,\tt: 814,\tdt: 143.407ms,\tSimpleTD/loss: 0.00974,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00149\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 28,\tT: 26,634,\tG: -21,\tavg_G: -20.3,\tt: 910,\tdt: 136.942ms,\tSimpleTD/loss: 0.00806,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00412\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 29,\tT: 27,421,\tG: -21,\tavg_G: -20.4,\tt: 786,\tdt: 156.750ms,\tSimpleTD/loss: 0.00239,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00214\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 30,\tT: 28,212,\tG: -21,\tavg_G: -20.5,\tt: 790,\tdt: 136.502ms,\tSimpleTD/loss: 0.00889,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00109\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 31,\tT: 29,222,\tG: -20,\tavg_G: -20.4,\tt: 1009,\tdt: 142.213ms,\tSimpleTD/loss: 0.0122,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00353\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 32,\tT: 30,264,\tG: -20,\tavg_G: -20.4,\tt: 1041,\tdt: 144.683ms,\tSimpleTD/loss: 0.0126,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00349\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 33,\tT: 31,131,\tG: -20,\tavg_G: -20.3,\tt: 866,\tdt: 135.671ms,\tSimpleTD/loss: 0.0084,\tEntropyRegularizer/entropy: 1.78,\tPPOClip/loss: -0.00193\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 34,\tT: 32,081,\tG: -20,\tavg_G: -20.3,\tt: 949,\tdt: 150.405ms,\tSimpleTD/loss: 0.0112,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00257\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 35,\tT: 32,979,\tG: -21,\tavg_G: -20.4,\tt: 897,\tdt: 134.902ms,\tSimpleTD/loss: 0.0138,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00145\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 36,\tT: 33,953,\tG: -20,\tavg_G: -20.3,\tt: 973,\tdt: 143.604ms,\tSimpleTD/loss: 0.0121,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00534\n",
      "[ppo|MainThread|TrainMonitor|INFO] ep: 37,\tT: 34,820,\tG: -20,\tavg_G: -20.3,\tt: 866,\tdt: 154.555ms,\tSimpleTD/loss: 0.0103,\tEntropyRegularizer/entropy: 1.77,\tPPOClip/loss: -0.00228\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while env.T < 3000000:\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(env.spec.max_episode_steps):\n",
    "        a, logp = pi_behavior(s, return_logp=True)\n",
    "        s_next, r, done, info = env.step(a)\n",
    "\n",
    "        # trace rewards and add transition to replay buffer\n",
    "        tracer.add(s, a, r, done, logp)\n",
    "        while tracer:\n",
    "            buffer.add(tracer.pop())\n",
    "\n",
    "        # learn\n",
    "        if len(buffer) >= buffer.capacity:\n",
    "            num_batches = int(4 * buffer.capacity / 32)  # 4 epochs per round\n",
    "            for _ in range(num_batches):\n",
    "                transition_batch = buffer.sample(32)\n",
    "                metrics_v, td_error = simpletd.update(transition_batch, return_td_error=True)\n",
    "                metrics_pi = ppo_clip.update(transition_batch, td_error)\n",
    "                env.record_metrics(metrics_v)\n",
    "                env.record_metrics(metrics_pi)\n",
    "\n",
    "            buffer.clear()\n",
    "\n",
    "            # sync target networks\n",
    "            pi_behavior.soft_update(pi, tau=0.1)\n",
    "            v_targ.soft_update(v, tau=0.1)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        s = s_next\n",
    "\n",
    "    # generate an animated GIF to see what's going on\n",
    "    if env.period(name='generate_gif', T_period=10000) and env.T > 50000:\n",
    "        T = env.T - env.T % 10000  # round to 10000s\n",
    "        coax.utils.generate_gif(\n",
    "            env=env, policy=pi, resize_to=(320, 420),\n",
    "            filepath=f\"./data/gifs/{name}/T{T:08d}.gif\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
